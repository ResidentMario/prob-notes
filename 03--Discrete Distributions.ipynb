{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **discrete probability mass function** (pmf) is defined as $P(X=x) = P(\\cup\\{s \\in S: X(s) = x\\})$.\n",
    "* The **cumulative distribution function** (cdf) is defined as $P(X \\leq x) = \\sum_{y: y \\leq x}P(y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **binomial theorem** states that $(x+y)^n=\\sum_{k=0}^n \\binom{n}{k} x^{n-k}y^k$. Proof is via induction: https://www.math.hmc.edu/calculus/tutorials/binomial_thm/induction.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **binomial distribution** is the probability distribution for the selection of some number of successes ($x$) from some number of trials ($n$), with each trial independently having some chance of success ($p$).\n",
    "* Binomial PDF, by definition: $P[X\\sim binom[n,p](\\cdot)=x] = \\binom{n}{k}(p)^k (1-p)^{n-k}$.\n",
    "* Binomial CDF, by definition: $P[X\\sim binom[n,p](\\cdot)\\leq x] = \\sum_{i=0}^x \\binom{n}{k}(p)^i (1-p)^{n-i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If $X \\sim binom[n,p](x)$ then $E[X] = np$. Proof:\n",
    "\n",
    "$$E[binom[n,p](x)] = \\sum x_k P(x_k)$$\n",
    "$$= \\sum_{k=1}^n (k)\\binom{n}{k}(p)^k (1-p)^{n-k}$$\n",
    "$$= \\sum_{k=1}^n (k)\\binom{n}{k}(p)^k(1-p)^{n-k} \\quad \\because \\binom{n}{0}(p)^0(1-p)^{n-0}(0) = 0$$\n",
    "$$\\text{ }$$\n",
    "$$\\text{Lemma: $(k)\\binom{n}{k} = (k)\\dfrac{n!}{k!(n-k)!} = \\dfrac{n!}{(k-1)!(n-k)!}$} = (n)\\dfrac{n-1}{(k-1)(n-k)!} = (n)\\binom{n-1}{k-1}$$\n",
    "$$\\text{ }$$\n",
    "$$\\text{Substituting this into the equation we find that:}$$\n",
    "$$\\therefore E[binom[n,p](x)] = \\sum_{k=1}^n n \\binom{n-1}{k-1}(p)^k (1-p)^{n-k}$$\n",
    "$$= n\\sum_{k=1}^n \\binom{n-1}{k-1}(p)^k (1-p)^{n-k}$$\n",
    "$$= np\\sum_{k=1}^n \\binom{n-1}{k-1}(p)^{k-1} (1-p)^{n-k}$$\n",
    "$$= np\\sum_{k=0}^n-1 \\binom{n}{k}(p)^{k} (1-p)^{n-k}$$\n",
    "$$= np(p+1-p) \\quad \\because \\text{ binomial theorem}$$\n",
    "$$= np$$\n",
    "$$\\text{QED.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If $X~binom[n,p](x)$ then $\\sigma_x^2 = npq$. Proof:\n",
    "\n",
    "$$\\text{Lemma: $E[X^2] = \\sum_{k=0}^n k^2 \\binom{n}{k} p^k q^{n-k}$}$$\n",
    "$$\\text{ }$$\n",
    "$$E[X^2] = \\sum_{k=0}^n k^2 \\binom{n}{k} p^k q^{n-k}$$\n",
    "$$= \\sum_{k=0}^n kn \\binom{n-1}{k-1}p^k q^{n-k} \\quad \\because k\\binom{n}{k} = n \\binom{n-1}{k-1}$$\n",
    "$$\\text{^ \"Binomial Factoring\" trick!}$$\n",
    "$$= np\\sum_{k=0}^n k \\binom{n-1}{k-1}p^{k-1}q^{n-k}$$\n",
    "$$= np\\sum_{k=1}^n k \\binom{n-1}{k-1}p^{k-1}q^{n-k} \\quad \\because \\text{term is 0 when $k=0$}$$\n",
    "$$\\text{Let $m=n-1$ and $j=k-1$. Then:}$$\n",
    "$$= np\\sum_{j=0}^m (j+1)\\binom{m}{j} p^j q^{m-j}$$\n",
    "$$= np\\sum_{j=0}^m j\\binom{m}{j} p^j q^{m-j} + np\\sum_{j=0}^m \\binom{m}{j} p^j q^{m-j}$$\n",
    "$$= np\\sum_{j=0}^m j\\binom{m}{j} p^j q^{m-j} + np(p+q) \\quad \\because \\text{Binomial Theorem}$$\n",
    "$$= np\\sum_{j=0}^m j\\binom{m}{j} p^j q^{m-j} + np$$\n",
    "$$= np\\sum_{j=0}^m m\\binom{m-1}{j-1} p^j q^{m-j} + np \\quad \\because k\\binom{n}{k} = n \\binom{n-1}{k-1}\\text{ (again)}$$\n",
    "$$= nmp^2\\sum_{j=0}^m \\binom{m=1}{j-1}p^{j-1}q^{m-1-(j-1}$$\n",
    "$$= mnp^2 + np \\quad \\because \\text{Binomial Theorem}$$\n",
    "$$= np(n-1)p + np$$\n",
    "$$= n^2p^2 + npq$$\n",
    "$$\\text{ }$$\n",
    "$$\\therefore \\sigma^2(binom[n,](x)) = n^2 p^2 - E[X]^2$$\n",
    "$$= n^2 p^2 + npq - (np)^2$$\n",
    "$$= np(np+q-np)$$\n",
    "$$= npq$$\n",
    "$$\\text{QED.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that the binomial distribution considers sampling with replacement. It can be used to approximate sampling without replacement, if the number of items is high enough and $x$ small enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **hypergeometric distribution** is the without-replacement equivalent to the binomial trial. It covers the more complex case when you have to account for the changes previous choices have on the probabilities of drawing from the remaining pot.\n",
    "* The binomial PDF is $P[X \\sim hypergeo[N, K, n](\\cdot) = x] = \\dfrac{\\binom{K}{x}\\binom{N-K}{n-x}}{\\binom{N}{n}}$; $N$ is population size, $K$ is total successes in the population, $n$ is the number of draws, and $x$ (the independent variable) is the number of successes desired to be drawn.\n",
    "* $E[hypergeo[N, K, n](\\cdot)] = \\frac{nK}{N}$. Proof:\n",
    "\n",
    "$$E[hypergeo[N, K, n](\\cdot)] = \\sum_{i=0}^n x \\frac{\\binom{K}{x}\\binom{N-k}{n-x}}{\\binom{N}{n}}$$\n",
    "$$= \\sum_{i=1}^n x \\frac{\\binom{K}{x}\\binom{N-k}{n-x}}{\\binom{N}{n}}\\quad\\because\\text{term is 0 at $x$}$$\n",
    "$$= \\sum_{i=1}^nx\\frac{\\frac{K}{x}\\binom{K-1}{x-1}\\binom{N-K}{n-x}}{\\frac{N}{n}\\binom{N-1}{n-1}}\\quad\\because \\binom{n}{K} = \\frac{n}{K}\\binom{n-1}{K-1}\\text{ (twice)}$$\n",
    "$$= \\frac{nK}{N}\\sum_{i=1}^nx\\frac{\\binom{K-1}{x-1}\\binom{N-K}{n-x}}{\\binom{N-1}{n-1}}$$\n",
    "$$\\text{Let $j=i-1$. Then:}$$\n",
    "$$= \\sum_{j=0}^n\\frac{\\binom{K-1}{j}\\binom{N-1-(K-1)}{n-1-(x-1)}}{\\binom{N-1}{n-1}}$$\n",
    "$$= \\frac{nk}{N}\\sum_{j=0}^n hypergeo[N, K-1, n-1](\\cdot)$$\n",
    "$$= \\frac{nK}{M}(1)\\quad\\because\\text{ right side is just a summation of the total hypergeometric}$$\n",
    "$$= \\frac{nK}{M}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\sigma^2(hypergeo[N, K,n](\\cdot)) = ???$. Proof:\n",
    "\n",
    "$$\\sigma^2(X \\sim hypergeo[N, K,n](\\cdot)) = E[X^2] - E[X]^2$$\n",
    "$$= E[X^2] - \\left(\\dfrac{nK}{N}\\right)^2$$\n",
    "$$= \\sum_{x=0}^n x^2 \\dfrac{\\binom{K}{x}\\binom{N-K}{n-x}}{\\binom{N}{n}} - \\left(\\dfrac{nK}{N}\\right)^2$$\n",
    "$$= \\sum_{x=0}^n x^2 \\dfrac{\\frac{K}{x}\\binom{K-1}{x-1}\\binom{N-K}{n-x}}{\\frac{N}{n}\\binom{N-1}{n-1}} - \\left(\\dfrac{nK}{N}\\right)^2 \\quad \\because \\binom{n}{k} = \\frac{n}{k}\\binom{n-1}{k-1}\\text{ (again)}$$\n",
    "$$= \\frac{Kn}{N}\\sum_{x=0}^n x \\dfrac{\\frac{K}{x}\\binom{K-1}{x-1}\\binom{N-K}{n-x}}{\\frac{N}{n}\\binom{N-1}{n-1}} - \\left(\\dfrac{nK}{N}\\right)^2$$\n",
    "$$\\text{ }$$\n",
    "$$\\text{ }$$\n",
    "$$= \\frac{Kn}{N} \\sum_{x=1}^n x \\dfrac{\\binom{K-1}{x-1}\\binom{N-K}{n-1-(x-1)}}{\\binom{N-1}{n-1}} - \\left(\\dfrac{nK}{N}\\right)^2$$\n",
    "$$\\text{Let $K'=K-1$, $x'=x-1$, $N'=N-1$, $n'=n-1$. Then:}$$\n",
    "$$= \\frac{Kn}{N}\\sum_{x'=0}^{n'}(x'+1)\\dfrac{\\binom{k'}{x'}\\binom{N'-k'}{n'-x'}}{\\binom{N'}{n'}} - \\left(\\dfrac{nK}{N}\\right)^2$$\n",
    "$$= \\frac{Kn}{N}(E[hypergeo[n'N'k'](\\cdot) + \\sum_{x'=0}^{n'}pmf(hypergeo[n',N',K'](\\cdot))) - \\left(\\dfrac{nK}{N}\\right)^2$$\n",
    "$$= \\frac{Kn}{N}\\left(\\frac{n'K'}{N'} + 1\\right) - \\left(\\dfrac{nK}{N}\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **Poisson distribution** covers the number of events which occur in an interval given the assumption that:\n",
    " * K is the number of times an event occurs in an interval and K can take\n",
    "values 0, 1, 2, â€¦\n",
    " * The occurrence of one event does not affect the probability that a second event will occur. That is, events occur independently.\n",
    " * The rate at which events occur is constant. The rate cannot be higher in some intervals and lower in other intervals.\n",
    " * Two events cannot occur at exactly the same instant.\n",
    " * The probability of an event in an interval is proportional to the length of the interval.\n",
    "* The previous distributions are easy to reason about and derive, with adequate combinatorical thinking. The provenance of the Possion processs is (to my young eyes) harder to understand. Proof of correctness:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Consider some short unit of time $\\delta t$, such that only one event may occur in this time period, or none at all. Then:\n",
    "\n",
    "$$P[\\delta t](1) = \\lambda \\delta t$$\n",
    "$$\\text{and } P[\\delta t](0) = 1-\\lambda \\delta t$$\n",
    "$$\\text{ }$$\n",
    "$$\\text{Consider $P[t+\\delta t](0)$. Assume events are independent. Then: }$$\n",
    "$$\\text{ }$$\n",
    " $$P[t+\\delta t](0) = P[t](0)P[\\delta t](0)$$\n",
    " $$= P[t](0)(1-\\lambda\\delta t)$$\n",
    " $$\\therefore \\frac{P[t+\\delta t](0) - P[t](0)}{\\delta t} = -\\lambda\n",
    " P[t](0)$$\n",
    " $$\\text{ }$$\n",
    " $$\\text{Let $\\delta t \\to 0$. Then this is a derivative:}$$\n",
    " $$\\text{ }$$\n",
    " $$\\delta t \\to 0 \\implies \\frac{dP}{dt} = -\\lambda P[t](0)$$\n",
    " $$\\therefore \\frac{dP}{P[t](0)} = -\\lambda dt$$\n",
    " $$\\therefore \\int \\frac{dP}{P[t](0)} = -\\int \\lambda dt$$\n",
    " $$\\therefore \\ln|P[t](0)| = -\\lambda t+c$$\n",
    " $$\\therefore P[t](0) = e^{-\\lambda t + c} = Ce^{-\\lambda t}$$\n",
    " $$\\text{ }$$\n",
    " $$\\text{We know the boundary condition P[0](0) = 1:}$$\n",
    " $$\\text{ }$$\n",
    " $$P[0](0) = 1 = Ce^{-\\lambda(0)} = C$$\n",
    " $$\\therefore P[t](0) = e^{-\\lambda t}$$\n",
    " $$\\text{ }$$\n",
    " $$\\text{Now consider $n$ events in time $t+\\lambda t$. This is given by the sum of terms:}$$\n",
    " $$\\text{ }$$\n",
    " $$P[t+\\delta t](n) = P[t](n)P[\\delta t](0) + P[t](n-1)P[\\delta t](1)$$\n",
    " $$=(1-\\lambda \\delta t)P[t](n) + (\\lambda \\delta t) P[t](n-1)$$\n",
    " $$=P[t](n) - (\\lambda \\delta t)P[t](n) + (\\lambda \\delta t)P[t](n-1)\n",
    "$$\n",
    " $$\\therefore \\frac{P[t+\\delta t](n) - P[t](n)}{\\lambda \\delta t} =\n",
    "\\lambda(P[t](n-1) - P[t](n))$$\n",
    " $$\\text{ }$$\n",
    " $$\\text{Once again taking $\\delta t \\to 0$:}$$\n",
    " $$\\text{ }$$\n",
    " $$\\therefore \\frac{dP}{dt} + \\lambda P[t](n) = \\lambda P[t](n-1)$$\n",
    " $$\\text{This is a first-order linear ODE with solution factor $\\mu (t)\n",
    " = e^{\\lambda t}$}.$$\n",
    " $$\\therefore \\frac{d}{dt}\\left[e^{\\lambda t}P[t](n)\\right] = e^{\\l\n",
    "ambda t}\\lambda P[t](n-1)$$\n",
    " $$\\text{ }$$\n",
    " $$\\text{We proceed by induction. Let $n=1$ be the base case. Suppose ou\n",
    "r formula is $P[t](n) = \\frac{(\\lambda t)^n}{n!}e^{-\\lambda t}$ Then:}$$\n",
    " $$\\text{ }$$\n",
    " $$\\frac{d}{dt}\\left[e^{\\lambda t}P[t](n)\\right] = e^{\\lambda t}P[t]\n",
    "(0) = \\lambda e^{\\lambda t}(e^{-\\lambda t})\\quad\\because P[t](0) = e^{-\\la\n",
    "mbda t}\\text{ (earlier)}$$\n",
    " $$= \\lambda$$\n",
    " $$\\therefore \\int \\frac{d}{dt}\\left[e^{\\lambda t}P[t](n)\\right] =\n",
    "e^{\\lambda t}P[t](1) = \\lambda t \\implies P[t](1) = \\lambda t e^{-\\lambda t\n",
    "}$$\n",
    " $$\\text{ }$$\n",
    " $$\\text{Base case is satisfied. Induct to $n$. Then taking case $n+1$:}\n",
    "$$\n",
    " $$\\text{ }$$\n",
    " $$\\frac{d}{dt}\\left[e^{\\lambda t}P[t](n+1)\\right] = e^{\\lambda t}\\\n",
    "lambda P[t](n) = e^{\\lambda t} \\lambda \\frac{(\\lambda t)^n}{n!}e^{-\\lambda\n",
    "t} = \\lambda \\frac{(\\lambda t)^n}{n!}$$\n",
    " $$\\therefore e^{\\lambda t}P[t](n+1) = \\int \\lambda \\frac{(\\lambda\n",
    "t)^n}{n!}dt = \\frac{(\\lambda t)^{n+1}}{(n+1)!} + C$$\n",
    " $$\\text{ }$$\n",
    " $$\\text{We know that $P[0](n+1) = 0$, which implies that $C$ = 0.}$$\n",
    " $$\\text{ }$$\n",
    " $$\\therefore P[t](n+1) = \\frac{(\\lambda t)^{n+1}}{(n+1)!}e^{-\\lambda\n",
    " t}$$\n",
    " $$\\text{QED.}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* $E[X\\sim Poission[\\lambda](\\cdot)] = \\lambda$. Proof:\n",
    "\n",
    "$$E[X] = \\sum_{x \\geq 0}xP[\\lambda](x)$$\n",
    "$$= \\sum_{x \\geq 0}x\\frac{1}{x!}\\lambda^x e^{-\\lambda}$$\n",
    "$$= \\sum_{x \\geq 1}x\\frac{1}{x!}\\lambda^x e^{-\\lambda}\\quad \\because \\text{ term is 0 when $x=0$}$$\n",
    "$$= \\lambda e^{-\\lambda}\\sum_{x \\geq 0}\\frac{1}{(x-1)!}\\lambda^{x-1}$$\n",
    "$$\\text{ }$$\n",
    "$$\\text{Letting $y=x-1$:}$$\n",
    "$$\\text{ }$$\n",
    "$$= \\lambda e^{-\\lambda}\\sum_{y \\geq 0} \\frac{1}{y!}\\lambda^y$$\n",
    "$$= \\lambda e^{-\\lambda}e^\\lambda \\quad \\because \\text{ $\\sum = e^{-\\lambda}$ via Taylor series expansion.}$$\n",
    "$$= \\lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\sigma^2[X\\sim Poission[\\lambda](\\cdot)] = \\lambda$. Proof:\n",
    "\n",
    "$$\\sigma^2[X] = E[X^2] - E[X]^2 = E[X^2] - \\lambda^2$$\n",
    "$$= \\sum_{x \\geq 0}x^2\\frac{1}{x!}\\lambda^x e^{-\\lambda} - \\lambda^2$$\n",
    "$$= \\sum_{x \\geq 1}x^2\\frac{1}{x!}\\lambda^x e^{-\\lambda} - \\lambda^2$$\n",
    "$$= \\lambda e^{-\\lambda} \\sum_{k\\geq 1} k \\frac{1}{(k-1)!}\\lambda^{k-1} - \\lambda^2$$\n",
    "$$= \\lambda e^{-\\lambda} \\left(\\sum_{k\\geq 1}(k-1)\\frac{1}{(k-1)!}\\lambda^{k-1}+\\sum_{k\\geq 1}\\frac{1}{(k-1)!}\\lambda^{k-1}\\right) - \\lambda^2$$\n",
    "$$= \\lambda e^{-\\lambda} \\left(\\lambda \\sum_{k\\geq 1}\\frac{1}{(k-2)!}\\lambda^{k-2}+\\sum_{k\\geq 1}\\frac{1}{(k-1)!}\\lambda^{k-1}\\right) - \\lambda^2$$\n",
    "$$\\text{ }$$\n",
    "$$\\text{Substitute $i=k-2$, $j=k-1$:}$$\n",
    "$$\\text{ }$$\n",
    "$$= \\lambda e^{-\\lambda} \\left(\\lambda \\sum_{k\\geq 1}\\frac{1}{(i)!}\\lambda^{i}+\\sum_{k\\geq 1}\\frac{1}{(j)!}\\lambda^{j}\\right) - \\lambda^2$$\n",
    "$$= \\lambda e^{-\\lambda}(\\lambda e^\\lambda + e^\\lambda) - \\lambda^2 \\quad \\because \\text{ $\\sum = e^{-\\lambda}$ via Taylor series expansion.}$$\n",
    "$$= \\lambda^2 + \\lambda - \\lambda^2$$\n",
    "$$= \\lambda$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The [Poisson limit theorem](https://en.wikipedia.org/wiki/Poisson_limit_theorem) holds that for sufficiently large $n$ and sufficiently small $p$, a binomial distribution with parameters $n$ and $p$ can be approximated by a Poisson with parameter $\\lambda = np$:\n",
    "\n",
    "  $$binom[n,p](k) \\to Poisson[\\lambda=np](k)$$\n",
    "\n",
    "  Given that the Poisson can easily be seen as a binomial with a practically infinite number of trials (this is invoked in its derivation), it's easy to see why $n \\to \\infty$ is necessary. As with the (I think, more intuitive) De Moivre-Laplace theorem (next, in 4), we prove that this approximation works for some constant $\\lambda(k)$:\n",
    "  \n",
    "  $$\\exists \\lambda(k).\\; \\forall k.\\; \\lim_{n \\to \\infty}binom[n,p](k) = Poisson[\\lambda](k)$$\n",
    "  \n",
    "  We hypothesize that $\\lambda = np$:\n",
    "  \n",
    "  $$\\forall k.\\; \\lim_{n \\to \\infty}binom[n,p](k) = Poisson[\\lambda = np](k)$$\n",
    "  \n",
    "  From here we proceed with a relatively straightforward proof.\n",
    "  \n",
    "  In order for this approximation to be good, then, $n$ must be high, and, correspondingly, $p$ must be low."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
